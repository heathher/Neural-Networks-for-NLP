{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ВАШЕ_ИМЯ_И_ФАМИЛИЯ = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение в обработку естественного языка. Наивная байесовская классификация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем данные. У нас есть три выборки: обучающая (train), валидационная (dev) и тестовая (test). На валидационной выборке тестируем классификатор и подбираем гиперпараметры. Результаты работы классификатора на тестовой выборке проверяются на сервере СompAI, метки для нее не предоставляются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '../FILIMDB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    Reads specified file, returns list of strings\n",
    "    :param file_name: file name in data_dir folder\n",
    "    :returns list of strings\n",
    "    \"\"\"\n",
    "    print('Loading %s' % file_name)\n",
    "    data_path = os.path.join(data_dir, file_name)\n",
    "    with open(data_path) as input_data:\n",
    "        lines = input_data.readlines()\n",
    "        lines = [l.strip() for l in lines]\n",
    "    \n",
    "    print('Loaded %d lines' % len(lines))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_texts, train_labels = load_data('train.texts'), load_data('train.labels')\n",
    "dev_texts, dev_labels = load_data('dev.texts'), load_data('dev.labels')\n",
    "test_texts = load_data('test.texts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на примеры негативных рецензий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_review(n, sentiment):\n",
    "    for _ in range(n):\n",
    "        idx = random.randrange(len(train_texts))\n",
    "        while train_labels[idx] != sentiment:\n",
    "            idx = random.randrange(len(train_texts))\n",
    "        print(train_texts[idx])\n",
    "        print(\"*\" * 100)\n",
    "    \n",
    "sample_review(3, 'neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А также положительных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_review(3, 'pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработка данных -- дело творческое, и может довольно сильно повлиять на результат. \n",
    "Чаще всего она включает следующие этапы:\n",
    "- Отделить пунктуацию от слов пробелами\n",
    "- Вычистить все лишнее: специальные символы, остатки html-разметки и т.д. При этом обычно не надо вычищать буквы с акцентами (à, á...). 'Мусорные' символы лучше не удалять, а заменять на пробелы, иначе слова могут склеиться.\n",
    "- Перевести буквы в нижний регистр\n",
    "\n",
    "Затем текст разбивается на токены - отдельные слова, знаки пунктуации, числа и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, какие символы встречаются у нас в текстах, выберем из них те, которые мы хотим оставить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char_set = set(ch for review in train_texts for ch in review)\n",
    "sorted_chars = sorted(list(char_set))\n",
    "print(''.join(sorted_chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь вы можете написать функцию, которая предобрабатывает текст и разбивает каждый отзыв на токены.\n",
    "\n",
    "Вам может пригодиться:\n",
    "\n",
    "Методы str.strip() и str.split()\n",
    "\n",
    "[Метод str.translate()](https://www.tutorialspoint.com/python/string_translate.htm)\n",
    "\n",
    "Для более сложной обработки используйте [регулярные выражения](https://ru.wikibooks.org/wiki/%D0%A0%D0%B5%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%BD%D1%8B%D0%B5_%D0%B2%D1%8B%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D1%8F). В питоне для этого есть модуль [re](https://docs.python.org/3.1/library/re.html)\n",
    "\n",
    "Подсказка. В python3 '\\w' в регулярных выражениях обозначает строчную букву любого алфавита, цифру или подчеркивание. \n",
    "Например, re.findall('w+') найдет все токены, состоящие из этих символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "ab7320fbd18a46aef4c7e93f0c8f22fc",
     "grade": false,
     "grade_id": "cell-812002070bd00bfc",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text and split it into the list of words\n",
    "    :param: text(str): movie review\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "tokenized_train_texts = [tokenize(r) for r in train_texts]\n",
    "tokenized_dev_texts = [tokenize(r) for r in dev_texts]\n",
    "tokenized_test_texts = [tokenize(r) for r in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "740b00c813cfaa08c05bb283319dabc8",
     "grade": true,
     "grade_id": "cell-64b8d9269836b45b",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "assert all(len(x) > 0 for x in tokenized_train_texts)\n",
    "unique_words = set(w for text in tokenized_train_texts for w in text)\n",
    "for word in [\"the\", \"and\", \"good\"]:\n",
    "    assert word in unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составим словарь из наиболее часто встречающихся в отзывах слов. Может быть полезно удалить из словаря стоп-слова - самые частые слова языка, не несущие важной информации для классификации (например \"you\", \"the\", \"i\" ...). Слова, не вошедшие в словарь, будем обозначать специальным символом \"UNK\"\n",
    "\n",
    "Количество слов в словаре - важный гиперпараметр классификатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d970cc569a2ad493a3d92a9fba19c93a",
     "grade": false,
     "grade_id": "cell-a41c4835ce5c6584",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 8000\n",
    "\n",
    "with open(\"stopwords.txt\", \"r\") as f:\n",
    "    STOPWORDS = map(lambda x: x.strip(), f.readlines())\n",
    "\n",
    "STOPWORDS = set(STOPWORDS)\n",
    "\n",
    "vocab = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "vocab.append(\"UNK\")\n",
    "vocab = set(vocab) # for faster searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a7665068e3b3cda2e5601be72e254176",
     "grade": true,
     "grade_id": "cell-4adf39f286b9c269",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert \"good\" in vocab\n",
    "assert all(x not in vocab for x in STOPWORDS)\n",
    "assert len(vocab) == VOCAB_SIZE + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь векторизуем отзывы: создадим defaultdict для двух классов, содержащих число вхождений каждого слова. Для этого напишите функцию, которая принимает на вход обзор и текущий defaultdict некоторого класса, который она затем обновляет, исходя из слов, содержащихся в обзоре. Не забывайте о том, что все слова рассматриваемого документа, не вошедшие в словарь, должны считаться одним и тем же словом, обозначаемом как \"UNK\".\n",
    "\n",
    "Выберите подходящий метод векторизации:\n",
    "\n",
    "    bag-of-words - подсчитываем, сколько раз встретилось каждое слово\n",
    "    binary bag-of-words - каждое слово в заданном документе надо учесть только один раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "227a6b7bd0a401a4ba54c0eda4c50985",
     "grade": false,
     "grade_id": "cell-86fccac8147cf63d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def update_class_word_counter(text, class_word_counter):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "positive_class_word_counter = defaultdict(int)\n",
    "negative_class_word_counter = defaultdict(int)\n",
    "\n",
    "for text, label in zip(tokenized_train_texts, train_labels):\n",
    "    if label == 'neg':\n",
    "        update_class_word_counter(text, negative_class_word_counter)\n",
    "    else:\n",
    "        update_class_word_counter(text, positive_class_word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6a898cf091702f5e9c27426d5e79cb13",
     "grade": true,
     "grade_id": "cell-e859a11cd9b95b59",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "for counter in [positive_class_word_counter, negative_class_word_counter]:\n",
    "    assert counter[\"think\"] > 1000\n",
    "    assert min(counter.values()) > 0\n",
    "    assert all(isinstance(x, int) for x in counter.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте наивный байесовский классификатор со сглаживанием (гиперпараметр ALPHA). Сохраните итоговые предсказания для обучающей, валидационной и тестовой выборок в переменные train_preds, dev_preds и test_preds соответственно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALPHA = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(texts, params=None):\n",
    "    \"\"\"\n",
    "    Classify items\n",
    "    :param texts: list of texts to classify\n",
    "    :param params: any params you need.\n",
    "    :return: list of predicted labels \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d55e8e0623ac4fc5aac47c770a048d64",
     "grade": false,
     "grade_id": "cell-768e769d5a9f0b7a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "train_preds = classify(train_texts)\n",
    "dev_preds = classify(dev_texts)\n",
    "test_preds = classify(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что классификатор дает осмысленные предсказания на обучающей выборке - ошибка прогноза должна быть в районе 15%. На валидационной выборке ошибка может оказаться немного больше, чем на обучающей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_score = sum([predict==label for predict, label in zip(train_preds, train_labels)]) / len(train_labels)\n",
    "print(\"Train score: \", train_score)\n",
    "dev_score = sum([predict==label for predict, label in zip(dev_preds, dev_labels)]) / len(dev_labels)\n",
    "print(\"Development score: \", dev_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "567cfdf5f64c781ea169953336a8007a",
     "grade": true,
     "grade_id": "cell-dfb8ad3c720d496d",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert(train_score > 0.82)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим файл с прогнозами и отправим его на CompAI сервер. Для этого понадобится положить файл compai_ilimdb_sentiment.py, полученный при регистрации, в директорию с этим блокнотом. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_file_name = \"preds.tsv\"\n",
    "with open(res_file_name, 'w') as outp:\n",
    "    for index, label in enumerate(train_preds):\n",
    "        print('train/%d\\t%s' % (index, label), file=outp)\n",
    "    for index, label in enumerate(dev_preds):\n",
    "        print('dev/%d\\t%s' % (index, label), file=outp)\n",
    "    for index, label in enumerate(test_preds):\n",
    "        print('test/%d\\t%s' % (index, label), file=outp)\n",
    "print('Predictions saved to %s' % res_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run compai_ilimdb_sentiment.py submit $res_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры способов улучшения модели классификации методом наивной байесовской классификации\n",
    "\n",
    "* Изменить размер словаря\n",
    "\n",
    "* Использовать биграммы помимо обычных слов\n",
    "\n",
    "* Воспользоваться [стеммингом](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%B5%D0%BC%D0%BC%D0%B8%D0%BD%D0%B3)\n",
    "\n",
    "* Подобрать параметр, отвечающий за сглаживание\n",
    "\n",
    "* Строить матрицу X на основе [TF-IDF](https://ru.wikipedia.org/wiki/TF-IDF) признаков\n",
    "\n",
    "* ???\n",
    "\n",
    "* PROFIT!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
